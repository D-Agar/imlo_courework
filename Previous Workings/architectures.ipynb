{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68ae0e633bb97602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:28.920779Z",
     "start_time": "2024-05-11T06:20:28.914421Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, v2\n",
    "from torch.nn import functional as F\n",
    "from scipy import io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b46d0dca59d957c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.000560Z",
     "start_time": "2024-05-11T06:20:28.996789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# set the selected device for the tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52118c3145c6b43b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cfa5503fe1b8c",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "After inspection of the dataset, we have PIL images. Therefore, we will convert these to Tensors.\n",
    "\n",
    "The values used for the normalisation of data were calculated from the ImageNet training datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd2b5524cdee4a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.011630Z",
     "start_time": "2024-05-11T06:20:29.002499Z"
    }
   },
   "outputs": [],
   "source": [
    "# We perform random transformations to better generalise the training dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), antialias=True),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # I have chosen not to use RandomResizeCrop as it can remove large parts of the flowers\n",
    "    transforms.ToTensor(),\n",
    "    # These are the values I have calculated\n",
    "    # transforms.Normalize(mean=[0.433, 0.382, 0.296], std=[0.259, 0.209, 0.221]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.433, 0.382, 0.296], std=[0.259, 0.209, 0.221])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.433, 0.382, 0.296],std=[0.259, 0.209, 0.221])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2aa7ea2e2ad2ee",
   "metadata": {},
   "source": [
    "### Downloading and splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548594f9d87289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.051220Z",
     "start_time": "2024-05-11T06:20:29.016635Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6765 6755 6768 ... 8026 8036 8041]]\n"
     ]
    }
   ],
   "source": [
    "# I will download the data from PyTorch's website and use the appropriate data loader\n",
    "train_dataset = datasets.Flowers102(\n",
    "    root='',\n",
    "    split=\"train\",\n",
    "    download=True,\n",
    "    transform=train_transform\n",
    "    # target_transform=Lambda(lambda y: F.one_hot(torch.FloatTensor(y), num_classes=102))\n",
    ")\n",
    "\n",
    "valid_dataset = datasets.Flowers102(\n",
    "    root='',\n",
    "    split=\"val\",\n",
    "    download=True,\n",
    "    transform=valid_transform,\n",
    "    # target_transform=Lambda(lambda y: torch.zeros(102, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Flowers102(\n",
    "    root='',\n",
    "    split=\"test\",\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Get the targets and ids\n",
    "image_labels = sio.loadmat(\"flowers-102/imagelabels\")\n",
    "setids = sio.loadmat(\"flowers-102/setid\")\n",
    "print(setids['trnid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcf967",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c9014",
   "metadata": {},
   "source": [
    "This architecture gave me an accuracy of 12% on the validation dataset. I will now attempt to use more channels and see if this improves the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d0d52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Identify a lot of features\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 600)\n",
    "        self.fc2 = nn.Linear(600, 300)\n",
    "        self.fc3 = nn.Linear(300, 102)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Batch is the 1st dimension, image dimensions are 2nd and 3rd\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82841f9",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc331fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making my Convolutional Neural Network\n",
    "class MyNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Feature Learning ###\n",
    "        # 3 Input channels\n",
    "        self.conv1 = nn.Conv2d(3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ### Classification ###\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        # After the 3rd convolutional layer, the filters will have dimensions 26x26\n",
    "        # 128 x 28 x 28 features\n",
    "        self.fc1 = nn.Linear(64*28*28, 2048)\n",
    "        # Classify into one of the flower categories\n",
    "        self.fc2 = nn.Linear(2048, 102)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers        \n",
    "        x = self.flatten(x)\n",
    "        x = nn.Dropout(0.2)(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c884be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a4cca9",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc77c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, downsample=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hold the residual, which will be added to the output\n",
    "        residual = self.downsample(x) if self.downsample is not None else x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class MyNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.cb1 = ConvBlock(16, 16, 3, 1, 1)\n",
    "        self.cb2 = ConvBlock(16, 32, 3, 1, 1)\n",
    "        self.cb3 = ConvBlock(32, 64, 3, 1, 1)\n",
    "        self.cb4 = ConvBlock(64, 128, 3, 1, 1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*14*14, 512)\n",
    "        self.fc2 = nn.Linear(512, 102)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.cb1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.cb2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.cb3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.cb4(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2e93f",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, downsample=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hold the residual, which will be added to the output\n",
    "        residual = self.downsample(x) if self.downsample is not None else x\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.cb1 = ConvBlock(16, 32, 3, 1, 1)\n",
    "        self.cb2 = ConvBlock(32, 64, 3, 1, 1)\n",
    "        self.cb3 = ConvBlock(64, 128, 3, 1, 1)\n",
    "        self.cb4 = ConvBlock(128, 256, 3, 1, 1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(256*14*14, 512)\n",
    "        self.fc2 = nn.Linear(512, 102)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.cb1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.cb2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.cb3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.cb4(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f1a6e",
   "metadata": {},
   "source": [
    "This aims to be slightly simpler by creating a bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927056bf",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*224*224, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 102)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.relu(self.conv6(x))\n",
    "        x = self.relu(self.conv7(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fff6dd5d54aaa20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.060537Z",
     "start_time": "2024-05-11T06:20:29.053227Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(weights=None).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697acb5a294fbb8e",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c93cf85eaa85fba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.066488Z",
     "start_time": "2024-05-11T06:20:29.063542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Targets don't have to be one-hot encoded\n",
    "# Model output (criterion input) is expected to contain un-normalised inputs\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347cd20c1ace324",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "998d709578f722b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.075049Z",
     "start_time": "2024-05-11T06:20:29.067493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Momentum pushes the optimiser towards the strongest gradient over multiple steps.\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6ac43b63b80d9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21a0427f1367a50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.082725Z",
     "start_time": "2024-05-11T06:20:29.079064Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, generator=torch.Generator(device=device), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8798d31c87f04732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.130152Z",
     "start_time": "2024-05-11T06:20:29.127277Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(valid_dataset, batch_size, shuffle=True, num_workers=2, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e97df8505c997e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.163829Z",
     "start_time": "2024-05-11T06:20:29.159157Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    \n",
    "    train_loss = 0.\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the loss/accuracy of the training set\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            train_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        correct /= len(dataloader.dataset)\n",
    "        train_loss /= len(dataloader)\n",
    "        print(f\"Training Error: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    return train_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "674f838b38c91b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.186457Z",
     "start_time": "2024-05-11T06:20:29.181834Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    return correct, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69abb35ac740ac74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.195378Z",
     "start_time": "2024-05-11T06:20:29.187462Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        self.patience = patience  # number of times to allow for no improvement before stopping the execution\n",
    "        self.min_delta = min_delta  # the minimum change to be counted as improvement\n",
    "        self.counter = 0  # count the number of times the validation accuracy not improving\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when validation loss is not decreased by the `min_delta` for `patience` times \n",
    "    def early_stop_check(self, validation_loss):\n",
    "        if ((validation_loss+self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # reset the counter if validation loss decreased at least by min_delta\n",
    "        elif ((validation_loss+self.min_delta) > self.min_validation_loss):\n",
    "            self.counter += 1 # increase the counter if validation loss is not decreased by the min_delta\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd6addc5f9c7f6a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:20:29.204064Z",
     "start_time": "2024-05-11T06:20:29.196295Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_model(model, train_loader, val_loader, criterion, optimizer, epoch, path):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_acc = 0.\n",
    "    # For enhanced GPU performance\n",
    "    if device == 'cuda':\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "    # early_stopper = EarlyStopping(patience=15, min_delta=0.01)\n",
    "    \n",
    "    print(\"[INFO] Training starting...\")\n",
    "    start_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loss, train_accuracy = train(train_loader, model, criterion, optimiser)\n",
    "        val_acc, val_loss = validate(valid_loader, model, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        # if early_stopper.early_stop_check(val_loss):             \n",
    "        #     break\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            \n",
    "    finish_time = time.time()\n",
    "    print(\"[INFO] Testing and training completed.\")\n",
    "    print(f\"Elapsed time: {math.floor((finish_time-start_time)/60)}:{math.floor((finish_time-start_time)%60)}\")\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0e32b5669c6dcbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:32:34.425228Z",
     "start_time": "2024-05-11T06:20:29.208534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training starting...\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training Error: \n",
      "Accuracy: 1.0%, Avg loss: 4.723095 \n",
      "\n",
      "Validation Error: \n",
      "Accuracy: 1.0%, Avg loss: 4.725154 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/vgg16.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m train_val_model(model, train_loader, valid_loader, criterion, optimiser, epochs, PATH)\n",
      "Cell \u001b[1;32mIn[41], line 17\u001b[0m, in \u001b[0;36mtrain_val_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, epoch, path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train(train_loader, model, criterion, optimiser)\n\u001b[0;32m     18\u001b[0m     val_acc, val_loss \u001b[38;5;241m=\u001b[39m validate(valid_loader, model, criterion)\n\u001b[0;32m     19\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[38], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimiser)\u001b[0m\n\u001b[0;32m     12\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 14\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m     17\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     adam(\n\u001b[0;32m    169\u001b[0m         params_with_grad,\n\u001b[0;32m    170\u001b[0m         grads,\n\u001b[0;32m    171\u001b[0m         exp_avgs,\n\u001b[0;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    174\u001b[0m         state_steps,\n\u001b[0;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m func(params,\n\u001b[0;32m    319\u001b[0m      grads,\n\u001b[0;32m    320\u001b[0m      exp_avgs,\n\u001b[0;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    323\u001b[0m      state_steps,\n\u001b[0;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\optim\\adam.py:567\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    565\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[0;32m    568\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[0;32m    570\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\coursework\\Lib\\site-packages\\torch\\utils\\_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "PATH = 'models/vgg16.pth'\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_val_model(model, train_loader, valid_loader, criterion, optimiser, epochs, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261369d7732c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the losses\n",
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bc88f5e3b30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the accuracy\n",
    "plt.plot(train_accuracies, label='Training')\n",
    "plt.plot(val_accuracies, label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e66b2362eabb88",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e568d5050fabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=True, num_workers=2, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c44af33b4480b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc = .0\n",
    "        for i, data in enumerate(test_loader):\n",
    "            X = data[0].to(device)\n",
    "            y = data[1].to(device)\n",
    "\n",
    "            predicted = model(X)\n",
    "            \n",
    "            # Check each image's prediction\n",
    "            acc += (predicted.argmax(1) == y).type(torch.float).sum().item()\n",
    "    model.train()\n",
    "    return acc/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68baa4456e8581a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T06:32:34.432590Z",
     "start_time": "2024-05-11T06:32:34.432590Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))\n",
    "test_acc = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a3d4a628d0f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
